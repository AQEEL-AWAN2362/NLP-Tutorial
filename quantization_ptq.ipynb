{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNULw5bPkZvXBKwqEQYOWHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AQEEL-AWAN2362/NLP-Tutorial/blob/main/quantization_ptq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4PuHZkUP8xa7"
      },
      "outputs": [],
      "source": [
        "# importing dependencies\n",
        "import torch\n",
        "from torch import nn  # for loss\n",
        "import torch.nn.functional as F   # for optimizer\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = make_moons(n_samples=5000,noise=0.5,random_state=42)"
      ],
      "metadata": {
        "id": "zbQTuny5-OIS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x,  \"shape of x: \" ,  x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KxXNOSW-OFL",
        "outputId": "190312f5-7545-4c2e-a8c2-83e903395643"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.64527536  1.38251014]\n",
            " [ 0.14514823 -0.32157033]\n",
            " [ 0.11945131  0.41631146]\n",
            " ...\n",
            " [ 0.6360473   0.66530771]\n",
            " [ 1.61542641 -0.24249711]\n",
            " [ 0.10599548  1.0899585 ]] shape of x:  (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
        "# plt.title(\"make_moons Dataset\")\n",
        "# plt.xlabel(\"Feature 1\")\n",
        "# plt.ylabel(\"Feature 2\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "VKmQM3pv-OCV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "x= torch.tensor(x, dtype=torch.float32)  # making ready for torch type\n",
        "y= torch.tensor(y.reshape(-1,1), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "qVoODGGK_sAS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DYBe_B-a_r8Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiating model with forward pass\n",
        "class Big_MLP(nn.Module):\n",
        " def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(2,128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,64)\n",
        "    self.fc4 = nn.Linear(64,32)\n",
        "    self.fc5 = nn.Linear(32,16)\n",
        "    self.fc6 = nn.Linear(16,8)\n",
        "    self.fc7 = nn.Linear(8,1)\n",
        "\n",
        " def forward(self,x):    # getting output of each layer by (input*weights)activation\n",
        "  x= F.relu(self.fc1(x))\n",
        "  x= F.relu(self.fc2(x))\n",
        "  x= F.relu(self.fc3(x))\n",
        "  x= F.relu(self.fc4(x))\n",
        "  x= F.relu(self.fc5(x))\n",
        "  x= F.relu(self.fc6(x))\n",
        "  x= torch.sigmoid(self.fc7(x))\n",
        "  return x"
      ],
      "metadata": {
        "id": "_VFrugDB_r6Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32= Big_MLP()"
      ],
      "metadata": {
        "id": "sET1Hpz-FlFZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking all learnable parameters(weights and biases) and name\n",
        "# for name, param in model_fp32.named_parameters():\n",
        "#   print(name, param.shape)"
      ],
      "metadata": {
        "id": "KXMPe0iIGV5i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backword pass (calculating loss and optimizing it with optimizer e.g adamW)\n",
        "loss_fn = nn.BCELoss()\n",
        "# optimizer = torch.optim.AdamW(model_fp32.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.Adam(model_fp32.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(2000):\n",
        "    model_fp32.train()  # train the model\n",
        "    y_pred = model_fp32(x_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # âœ… Must be indented inside the loop\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\") # tensor(0.3478, grad_fn=<MseLossBackward>) so to convert it into python float we use .item()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EjIcOMSHV3X",
        "outputId": "5baa5543-6763-4acf-fcd2-888a5f472573"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6937\n",
            "Epoch 1000 | Loss: 0.3649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "def accuracy(model, x, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        preds = model(x)\n",
        "\n",
        "        preds = (preds > 0.5).float()\n",
        "\n",
        "        return (preds == y).float().mean().item() # We compare predicted == y_train to count how many are correct.\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy(model_fp32, x_test, y_test) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLrXWJSUHV1N",
        "outputId": "835ba581-4692-4bfd-8293-b456d9723672"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **QUANTIZED THE MODEL NOW**\n"
      ],
      "metadata": {
        "id": "zBru_vYOiL6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.quantization import quantize_dynamic # weights only quantized"
      ],
      "metadata": {
        "id": "NgzBfqQPHVyg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# real dynamic quantization\n",
        "model_int8 = quantize_dynamic(\n",
        "    model_fp32,   # original model\n",
        "    {nn.Linear},  # Which layers to quantize\n",
        "    dtype= torch.qint8   # quantized type\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C98CLXU_HVwl",
        "outputId": "9a7b31e1-dd71-46f5-8ac5-20c4d83debb8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-912045234.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = quantize_dynamic(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_int8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHIYWUyjb1m",
        "outputId": "25c3c3b4-e600-41a0-c13b-b57db2902f29"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Big_MLP(\n",
              "  (fc1): DynamicQuantizedLinear(in_features=2, out_features=128, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc2): DynamicQuantizedLinear(in_features=128, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc3): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc4): DynamicQuantizedLinear(in_features=64, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc5): DynamicQuantizedLinear(in_features=32, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc6): DynamicQuantizedLinear(in_features=16, out_features=8, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc7): DynamicQuantizedLinear(in_features=8, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INT8 Quantized Accuracy:\", accuracy(model_int8, x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q60Xz-CLjbxT",
        "outputId": "a6623612-02fd-46b7-e59a-c6166edc2ce1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INT8 Quantized Accuracy: 0.6520000100135803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduced parameters after dynamic quantization\n",
        "print(\"Before:\", sum(p.numel() for p in model_fp32.parameters()))\n",
        "print(\"After:\", sum(p.numel() for p in model_int8.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRVOTa8xo2Cy",
        "outputId": "2333f740-d667-4c7d-ad7b-126fdd4f89ee"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 15553\n",
            "After: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "torch.save(model_fp32.state_dict(), \"model_fp32.pt\")\n",
        "torch.save(model_int8.state_dict(), \"model_dynamic_int8.pt\")"
      ],
      "metadata": {
        "id": "dk4pNvdtjbu3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FP32 model size (MB):\", os.path.getsize(\"model_fp32.pt\") / 1e6)\n",
        "print(\"INT8 model size (MB):\", os.path.getsize(\"model_dynamic_int8.pt\") / 1e6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we91vgzAkXEs",
        "outputId": "9cc2ff7a-dced-4032-8d4f-0b0e91fb414e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 model size (MB): 0.067317\n",
            "INT8 model size (MB): 0.026373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCnDljZwobW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}